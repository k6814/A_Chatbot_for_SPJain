{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv\n",
      "train.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\",\"input\"]).decode(\"utf8\"))\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import datetime\n",
    "import operator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import plot, show, subplot, specgram, imshow, savefig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/houm/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2714: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: X_train: (404290, 6), X_test: (3563475, 3)\n",
      "Features processing, be patient...\n"
     ]
    }
   ],
   "source": [
    "RS = 123457\n",
    "ROUNDS = 190\n",
    "\n",
    "print(\"Started\")\n",
    "np.random.seed(RS)\n",
    "input_folder = 'input/'\n",
    "\n",
    "def train_xgb(X, y, params):\n",
    "    print(\"Will train XGB for {} rounds, RandomSeed: {}\".format(ROUNDS, RS))\n",
    "    x, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RS)\n",
    "    xg_train = lgb.Dataset(x, label=y_train)\n",
    "    xg_val = lgb.Dataset(X_val, label=y_val)\n",
    "    watchlist  = [xg_val]\n",
    "    clf = lgb.train(params, xg_train, ROUNDS, watchlist)\n",
    "    return clf\n",
    "\n",
    "\n",
    "def predict_xgb(clr, X_test):\n",
    "    return clr.predict(X_test)\n",
    "\n",
    "def add_word_count(x, df, word):\n",
    "    x['q1_' + word] = df['question1'].apply(lambda x: (word in str(x).lower())*1)\n",
    "    x['q2_' + word] = df['question2'].apply(lambda x: (word in str(x).lower())*1)\n",
    "    x[word + '_both'] = x['q1_' + word] * x['q2_' + word]\n",
    "\n",
    "# def main():\n",
    "param = {}\n",
    "param['learning_rate'] = 0.14\n",
    "param['boosting_type'] = 'dart'\n",
    "param['objective'] = 'binary'\n",
    "param['metric'] = 'binary_logloss'\n",
    "param['sub_feature'] = 0.5\n",
    "param['num_leaves'] = 512\n",
    "param['min_data'] = 50\n",
    "param['min_hessian'] = 1\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(input_folder + 'train.csv')\n",
    "df_test  = pd.read_csv(input_folder + 'test.csv')\n",
    "\n",
    "print(\"Original data: X_train: {}, X_test: {}\".format(df_train.shape, df_test.shape))\n",
    "\n",
    "print(\"Features processing, be patient...\")\n",
    "\n",
    "# If a word appears only once, we ignore it completely (likely a typo)\n",
    "# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    return 0 if count < min_count else 1 / (count + eps)\n",
    "\n",
    "train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "def word_shares(row):\n",
    "    q1_list = str(row['question1']).lower().split()\n",
    "    q1 = set(q1_list)\n",
    "    q1words = q1.difference(stops)\n",
    "    if len(q1words) == 0:\n",
    "        return '0:0:0:0:0:0:0:0'\n",
    "\n",
    "    q2_list = str(row['question2']).lower().split()\n",
    "    q2 = set(q2_list)\n",
    "    q2words = q2.difference(stops)\n",
    "    if len(q2words) == 0:\n",
    "        return '0:0:0:0:0:0:0:0'\n",
    "\n",
    "    words_hamming = sum(1 for i in zip(q1_list, q2_list) if i[0]==i[1])/max(len(q1_list), len(q2_list))\n",
    "\n",
    "    q1stops = q1.intersection(stops)\n",
    "    q2stops = q2.intersection(stops)\n",
    "\n",
    "    q1_2gram = set([i for i in zip(q1_list, q1_list[1:])])\n",
    "    q2_2gram = set([i for i in zip(q2_list, q2_list[1:])])\n",
    "\n",
    "    shared_2gram = q1_2gram.intersection(q2_2gram)\n",
    "\n",
    "    shared_words = q1words.intersection(q2words)\n",
    "    shared_weights = [weights.get(w, 0) for w in shared_words]\n",
    "    q1_weights = [weights.get(w, 0) for w in q1words]\n",
    "    q2_weights = [weights.get(w, 0) for w in q2words]\n",
    "    total_weights = q1_weights + q1_weights\n",
    "\n",
    "    R1 = np.sum(shared_weights) / np.sum(total_weights) #tfidf share\n",
    "    R2 = len(shared_words) / (len(q1words) + len(q2words) - len(shared_words)) #count share\n",
    "    R31 = len(q1stops) / len(q1words) #stops in q1\n",
    "    R32 = len(q2stops) / len(q2words) #stops in q2\n",
    "    Rcosine_denominator = (np.sqrt(np.dot(q1_weights,q1_weights))*np.sqrt(np.dot(q2_weights,q2_weights)))\n",
    "    Rcosine = np.dot(shared_weights, shared_weights)/Rcosine_denominator\n",
    "    if len(q1_2gram) + len(q2_2gram) == 0:\n",
    "        R2gram = 0\n",
    "    else:\n",
    "        R2gram = len(shared_2gram) / (len(q1_2gram) + len(q2_2gram))\n",
    "    return '{}:{}:{}:{}:{}:{}:{}:{}'.format(R1, R2, len(shared_words), R31, R32, R2gram, Rcosine, words_hamming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/houm/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  if __name__ == '__main__':\n",
      "/home/houm/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:85: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "/home/houm/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:90: RuntimeWarning: invalid value encountered in divide\n",
      "/home/houm/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:85: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0846a145b352>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'diff_len'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'len_q1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'len_q2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'caps_count_q1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'caps_count_q2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'diff_caps'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'caps_count_q1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'caps_count_q2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/houm/anaconda2/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0846a145b352>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'diff_len'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'len_q1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'len_q2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'caps_count_q1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'caps_count_q2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'diff_caps'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'caps_count_q1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'caps_count_q2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0846a145b352>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'diff_len'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'len_q1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'len_q2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'caps_count_q1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'caps_count_q2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'diff_caps'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'caps_count_q1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'caps_count_q2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.concat([df_train, df_test])\n",
    "df['word_shares'] = df.apply(word_shares, axis=1, raw=True)\n",
    "\n",
    "x = pd.DataFrame()\n",
    "\n",
    "x['word_match']       = df['word_shares'].apply(lambda x: float(x.split(':')[0]))\n",
    "x['word_match_2root'] = np.sqrt(x['word_match'])\n",
    "x['tfidf_word_match'] = df['word_shares'].apply(lambda x: float(x.split(':')[1]))\n",
    "x['shared_count']     = df['word_shares'].apply(lambda x: float(x.split(':')[2]))\n",
    "\n",
    "x['stops1_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[3]))\n",
    "x['stops2_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[4]))\n",
    "x['shared_2gram']     = df['word_shares'].apply(lambda x: float(x.split(':')[5]))\n",
    "x['cosine']           = df['word_shares'].apply(lambda x: float(x.split(':')[6]))\n",
    "x['words_hamming']    = df['word_shares'].apply(lambda x: float(x.split(':')[7]))\n",
    "x['diff_stops_r']     = x['stops1_ratio'] - x['stops2_ratio']\n",
    "\n",
    "x['len_q1'] = df['question1'].apply(lambda x: len(str(x)))\n",
    "x['len_q2'] = df['question2'].apply(lambda x: len(str(x)))\n",
    "x['diff_len'] = x['len_q1'] - x['len_q2']\n",
    "\n",
    "x['caps_count_q1'] = df['question1'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "x['caps_count_q2'] = df['question2'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "x['diff_caps'] = x['caps_count_q1'] - x['caps_count_q2']\n",
    "\n",
    "x['len_char_q1'] = df['question1'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "x['len_char_q2'] = df['question2'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "x['diff_len_char'] = x['len_char_q1'] - x['len_char_q2']\n",
    "\n",
    "x['len_word_q1'] = df['question1'].apply(lambda x: len(str(x).split()))\n",
    "x['len_word_q2'] = df['question2'].apply(lambda x: len(str(x).split()))\n",
    "x['diff_len_word'] = x['len_word_q1'] - x['len_word_q2']\n",
    "\n",
    "x['avg_world_len1'] = x['len_char_q1'] / x['len_word_q1']\n",
    "x['avg_world_len2'] = x['len_char_q2'] / x['len_word_q2']\n",
    "x['diff_avg_word'] = x['avg_world_len1'] - x['avg_world_len2']\n",
    "\n",
    "x['exactly_same'] = (df['question1'] == df['question2']).astype(int)\n",
    "x['duplicated'] = df.duplicated(['question1','question2']).astype(int)\n",
    "add_word_count(x, df,'how')\n",
    "add_word_count(x, df,'what')\n",
    "add_word_count(x, df,'which')\n",
    "add_word_count(x, df,'who')\n",
    "add_word_count(x, df,'where')\n",
    "add_word_count(x, df,'when')\n",
    "add_word_count(x, df,'why')\n",
    "\n",
    "print(x.columns)\n",
    "print(x.describe())\n",
    "\n",
    "feature_names = list(x.columns.values)\n",
    "print(\"Features: {}\".format(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling started for proportion: 0\n",
      "Oversampling done, new proportion: 0\n",
      "Training data: X_train: (404290, 48), Y_train: 404290, X_test: (3563475, 48)\n",
      "Will train XGB for 190 rounds, RandomSeed: 123457\n",
      "[1]\tvalid_0's binary_logloss: 0.63823\n",
      "[2]\tvalid_0's binary_logloss: 0.598742\n",
      "[3]\tvalid_0's binary_logloss: 0.586783\n",
      "[4]\tvalid_0's binary_logloss: 0.576888\n",
      "[5]\tvalid_0's binary_logloss: 0.569556\n",
      "[6]\tvalid_0's binary_logloss: 0.545858\n",
      "[7]\tvalid_0's binary_logloss: 0.540754\n",
      "[8]\tvalid_0's binary_logloss: 0.548879\n",
      "[9]\tvalid_0's binary_logloss: 0.527991\n",
      "[10]\tvalid_0's binary_logloss: 0.51203\n",
      "[11]\tvalid_0's binary_logloss: 0.508764\n",
      "[12]\tvalid_0's binary_logloss: 0.506748\n",
      "[13]\tvalid_0's binary_logloss: 0.494583\n",
      "[14]\tvalid_0's binary_logloss: 0.491664\n",
      "[15]\tvalid_0's binary_logloss: 0.489297\n",
      "[16]\tvalid_0's binary_logloss: 0.480207\n",
      "[17]\tvalid_0's binary_logloss: 0.478681\n",
      "[18]\tvalid_0's binary_logloss: 0.476774\n",
      "[19]\tvalid_0's binary_logloss: 0.475277\n",
      "[20]\tvalid_0's binary_logloss: 0.474317\n",
      "[21]\tvalid_0's binary_logloss: 0.472473\n",
      "[22]\tvalid_0's binary_logloss: 0.471208\n",
      "[23]\tvalid_0's binary_logloss: 0.470287\n",
      "[24]\tvalid_0's binary_logloss: 0.469241\n",
      "[25]\tvalid_0's binary_logloss: 0.461881\n",
      "[26]\tvalid_0's binary_logloss: 0.460997\n",
      "[27]\tvalid_0's binary_logloss: 0.459791\n",
      "[28]\tvalid_0's binary_logloss: 0.460767\n",
      "[29]\tvalid_0's binary_logloss: 0.455542\n",
      "[30]\tvalid_0's binary_logloss: 0.450497\n",
      "[31]\tvalid_0's binary_logloss: 0.450709\n",
      "[32]\tvalid_0's binary_logloss: 0.450183\n",
      "[33]\tvalid_0's binary_logloss: 0.446227\n",
      "[34]\tvalid_0's binary_logloss: 0.445579\n",
      "[35]\tvalid_0's binary_logloss: 0.446872\n",
      "[36]\tvalid_0's binary_logloss: 0.44738\n",
      "[37]\tvalid_0's binary_logloss: 0.446809\n",
      "[38]\tvalid_0's binary_logloss: 0.446278\n",
      "[39]\tvalid_0's binary_logloss: 0.445667\n",
      "[40]\tvalid_0's binary_logloss: 0.445162\n",
      "[41]\tvalid_0's binary_logloss: 0.446486\n",
      "[42]\tvalid_0's binary_logloss: 0.446127\n",
      "[43]\tvalid_0's binary_logloss: 0.446389\n",
      "[44]\tvalid_0's binary_logloss: 0.442514\n",
      "[45]\tvalid_0's binary_logloss: 0.442111\n",
      "[46]\tvalid_0's binary_logloss: 0.44407\n",
      "[47]\tvalid_0's binary_logloss: 0.443336\n",
      "[48]\tvalid_0's binary_logloss: 0.443408\n",
      "[49]\tvalid_0's binary_logloss: 0.445119\n",
      "[50]\tvalid_0's binary_logloss: 0.445465\n",
      "[51]\tvalid_0's binary_logloss: 0.442049\n",
      "[52]\tvalid_0's binary_logloss: 0.439028\n",
      "[53]\tvalid_0's binary_logloss: 0.439846\n",
      "[54]\tvalid_0's binary_logloss: 0.437089\n",
      "[55]\tvalid_0's binary_logloss: 0.43481\n",
      "[56]\tvalid_0's binary_logloss: 0.436578\n",
      "[57]\tvalid_0's binary_logloss: 0.434678\n",
      "[58]\tvalid_0's binary_logloss: 0.436065\n",
      "[59]\tvalid_0's binary_logloss: 0.437267\n",
      "[60]\tvalid_0's binary_logloss: 0.436875\n",
      "[61]\tvalid_0's binary_logloss: 0.438019\n",
      "[62]\tvalid_0's binary_logloss: 0.437382\n",
      "[63]\tvalid_0's binary_logloss: 0.436856\n",
      "[64]\tvalid_0's binary_logloss: 0.437724\n",
      "[65]\tvalid_0's binary_logloss: 0.438549\n",
      "[66]\tvalid_0's binary_logloss: 0.43633\n",
      "[67]\tvalid_0's binary_logloss: 0.433885\n",
      "[68]\tvalid_0's binary_logloss: 0.433492\n",
      "[69]\tvalid_0's binary_logloss: 0.434546\n",
      "[70]\tvalid_0's binary_logloss: 0.435123\n",
      "[71]\tvalid_0's binary_logloss: 0.435541\n",
      "[72]\tvalid_0's binary_logloss: 0.435103\n",
      "[73]\tvalid_0's binary_logloss: 0.432771\n",
      "[74]\tvalid_0's binary_logloss: 0.433193\n",
      "[75]\tvalid_0's binary_logloss: 0.432964\n",
      "[76]\tvalid_0's binary_logloss: 0.432933\n",
      "[77]\tvalid_0's binary_logloss: 0.433554\n",
      "[78]\tvalid_0's binary_logloss: 0.43436\n",
      "[79]\tvalid_0's binary_logloss: 0.432145\n",
      "[80]\tvalid_0's binary_logloss: 0.430416\n",
      "[81]\tvalid_0's binary_logloss: 0.430815\n",
      "[82]\tvalid_0's binary_logloss: 0.429046\n",
      "[83]\tvalid_0's binary_logloss: 0.429416\n",
      "[84]\tvalid_0's binary_logloss: 0.429972\n",
      "[85]\tvalid_0's binary_logloss: 0.430641\n",
      "[86]\tvalid_0's binary_logloss: 0.430051\n",
      "[87]\tvalid_0's binary_logloss: 0.42961\n",
      "[88]\tvalid_0's binary_logloss: 0.430071\n",
      "[89]\tvalid_0's binary_logloss: 0.430192\n",
      "[90]\tvalid_0's binary_logloss: 0.430747\n",
      "[91]\tvalid_0's binary_logloss: 0.431601\n",
      "[92]\tvalid_0's binary_logloss: 0.429814\n",
      "[93]\tvalid_0's binary_logloss: 0.429507\n",
      "[94]\tvalid_0's binary_logloss: 0.430154\n",
      "[95]\tvalid_0's binary_logloss: 0.430683\n",
      "[96]\tvalid_0's binary_logloss: 0.431037\n",
      "[97]\tvalid_0's binary_logloss: 0.430741\n",
      "[98]\tvalid_0's binary_logloss: 0.431258\n",
      "[99]\tvalid_0's binary_logloss: 0.429342\n",
      "[100]\tvalid_0's binary_logloss: 0.428766\n",
      "[101]\tvalid_0's binary_logloss: 0.429342\n",
      "[102]\tvalid_0's binary_logloss: 0.42949\n",
      "[103]\tvalid_0's binary_logloss: 0.429992\n",
      "[104]\tvalid_0's binary_logloss: 0.429525\n",
      "[105]\tvalid_0's binary_logloss: 0.430059\n",
      "[106]\tvalid_0's binary_logloss: 0.430828\n",
      "[107]\tvalid_0's binary_logloss: 0.431282\n",
      "[108]\tvalid_0's binary_logloss: 0.430776\n",
      "[109]\tvalid_0's binary_logloss: 0.431128\n",
      "[110]\tvalid_0's binary_logloss: 0.431827\n",
      "[111]\tvalid_0's binary_logloss: 0.42972\n",
      "[112]\tvalid_0's binary_logloss: 0.428154\n",
      "[113]\tvalid_0's binary_logloss: 0.427678\n",
      "[114]\tvalid_0's binary_logloss: 0.426406\n",
      "[115]\tvalid_0's binary_logloss: 0.426097\n",
      "[116]\tvalid_0's binary_logloss: 0.42569\n",
      "[117]\tvalid_0's binary_logloss: 0.426106\n",
      "[118]\tvalid_0's binary_logloss: 0.425129\n",
      "[119]\tvalid_0's binary_logloss: 0.425456\n",
      "[120]\tvalid_0's binary_logloss: 0.425827\n",
      "[121]\tvalid_0's binary_logloss: 0.426263\n",
      "[122]\tvalid_0's binary_logloss: 0.426559\n",
      "[123]\tvalid_0's binary_logloss: 0.425218\n",
      "[124]\tvalid_0's binary_logloss: 0.424079\n",
      "[125]\tvalid_0's binary_logloss: 0.424434\n",
      "[126]\tvalid_0's binary_logloss: 0.424953\n",
      "[127]\tvalid_0's binary_logloss: 0.424005\n",
      "[128]\tvalid_0's binary_logloss: 0.42431\n",
      "[129]\tvalid_0's binary_logloss: 0.423753\n",
      "[130]\tvalid_0's binary_logloss: 0.422733\n",
      "[131]\tvalid_0's binary_logloss: 0.423047\n",
      "[132]\tvalid_0's binary_logloss: 0.422657\n",
      "[133]\tvalid_0's binary_logloss: 0.421774\n",
      "[134]\tvalid_0's binary_logloss: 0.422182\n",
      "[135]\tvalid_0's binary_logloss: 0.422032\n",
      "[136]\tvalid_0's binary_logloss: 0.422516\n",
      "[137]\tvalid_0's binary_logloss: 0.422767\n",
      "[138]\tvalid_0's binary_logloss: 0.422635\n",
      "[139]\tvalid_0's binary_logloss: 0.421831\n",
      "[140]\tvalid_0's binary_logloss: 0.422202\n",
      "[141]\tvalid_0's binary_logloss: 0.422443\n",
      "[142]\tvalid_0's binary_logloss: 0.422734\n",
      "[143]\tvalid_0's binary_logloss: 0.421983\n",
      "[144]\tvalid_0's binary_logloss: 0.422239\n",
      "[145]\tvalid_0's binary_logloss: 0.421841\n",
      "[146]\tvalid_0's binary_logloss: 0.422288\n",
      "[147]\tvalid_0's binary_logloss: 0.422606\n",
      "[148]\tvalid_0's binary_logloss: 0.423026\n",
      "[149]\tvalid_0's binary_logloss: 0.422177\n",
      "[150]\tvalid_0's binary_logloss: 0.421861\n",
      "[151]\tvalid_0's binary_logloss: 0.42225\n",
      "[152]\tvalid_0's binary_logloss: 0.422537\n",
      "[153]\tvalid_0's binary_logloss: 0.422353\n",
      "[154]\tvalid_0's binary_logloss: 0.422595\n",
      "[155]\tvalid_0's binary_logloss: 0.422506\n",
      "[156]\tvalid_0's binary_logloss: 0.422778\n",
      "[157]\tvalid_0's binary_logloss: 0.42296\n",
      "[158]\tvalid_0's binary_logloss: 0.423285\n",
      "[159]\tvalid_0's binary_logloss: 0.423677\n",
      "[160]\tvalid_0's binary_logloss: 0.424028\n",
      "[161]\tvalid_0's binary_logloss: 0.42294\n",
      "[162]\tvalid_0's binary_logloss: 0.423139\n",
      "[163]\tvalid_0's binary_logloss: 0.422261\n",
      "[164]\tvalid_0's binary_logloss: 0.421333\n",
      "[165]\tvalid_0's binary_logloss: 0.420587\n",
      "[166]\tvalid_0's binary_logloss: 0.420327\n",
      "[167]\tvalid_0's binary_logloss: 0.419643\n",
      "[168]\tvalid_0's binary_logloss: 0.41917\n",
      "[169]\tvalid_0's binary_logloss: 0.418789\n",
      "[170]\tvalid_0's binary_logloss: 0.419043\n",
      "[171]\tvalid_0's binary_logloss: 0.418675\n",
      "[172]\tvalid_0's binary_logloss: 0.418941\n",
      "[173]\tvalid_0's binary_logloss: 0.418537\n",
      "[174]\tvalid_0's binary_logloss: 0.418097\n",
      "[175]\tvalid_0's binary_logloss: 0.418063\n",
      "[176]\tvalid_0's binary_logloss: 0.418202\n",
      "[177]\tvalid_0's binary_logloss: 0.41839\n",
      "[178]\tvalid_0's binary_logloss: 0.418614\n",
      "[179]\tvalid_0's binary_logloss: 0.418144\n",
      "[180]\tvalid_0's binary_logloss: 0.418341\n",
      "[181]\tvalid_0's binary_logloss: 0.417874\n",
      "[182]\tvalid_0's binary_logloss: 0.41771\n",
      "[183]\tvalid_0's binary_logloss: 0.417557\n",
      "[184]\tvalid_0's binary_logloss: 0.417202\n",
      "[185]\tvalid_0's binary_logloss: 0.41736\n",
      "[186]\tvalid_0's binary_logloss: 0.417258\n",
      "[187]\tvalid_0's binary_logloss: 0.417166\n",
      "[188]\tvalid_0's binary_logloss: 0.416916\n",
      "[189]\tvalid_0's binary_logloss: 0.417091\n",
      "[190]\tvalid_0's binary_logloss: 0.417288\n"
     ]
    }
   ],
   "source": [
    "x_train = x[:df_train.shape[0]]\n",
    "x_test  = x[df_train.shape[0]:]\n",
    "y_train = df_train['is_duplicate'].values\n",
    "del x, df_train\n",
    "\n",
    "if 1: # Now we oversample the negative class - on your own risk of overfitting!\n",
    "    pos_train = x_train[y_train == 1]\n",
    "    neg_train = x_train[y_train == 0]\n",
    "\n",
    "    print(\"Oversampling started for proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "    p = 0.165\n",
    "    scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "    while scale > 1:\n",
    "        neg_train = pd.concat([neg_train, neg_train])\n",
    "        scale -=1\n",
    "    neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "    print(\"Oversampling done, new proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "\n",
    "    x_train = pd.concat([pos_train, neg_train])\n",
    "    y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "    del pos_train, neg_train\n",
    "\n",
    "print(\"Training data: X_train: {}, Y_train: {}, X_test: {}\".format(x_train.shape, len(y_train), x_test.shape))\n",
    "clr = train_xgb(x_train.fillna(0), y_train, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'finalized_model.sav'\n",
    "# pickle.dump(clr, open(filename, 'wb'))\n",
    "\n",
    "with open(filename, 'rb') as f:\n",
    "    clr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=pd.read_csv(\"sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_shares(row):\n",
    "    q1_list = str(row['question1']).lower().split()\n",
    "    q1 = set(q1_list)\n",
    "    q1words = q1.difference(stops)\n",
    "    if len(q1words) == 0:\n",
    "        return '0:0:0:0:0:0:0:0'\n",
    "\n",
    "    q2_list = str(row['question2']).lower().split()\n",
    "    q2 = set(q2_list)\n",
    "    q2words = q2.difference(stops)\n",
    "    if len(q2words) == 0:\n",
    "        return '0:0:0:0:0:0:0:0'\n",
    "\n",
    "    words_hamming = sum(1 for i in zip(q1_list, q2_list) if i[0]==i[1])/max(len(q1_list), len(q2_list))\n",
    "\n",
    "    q1stops = q1.intersection(stops)\n",
    "    q2stops = q2.intersection(stops)\n",
    "\n",
    "    q1_2gram = set([i for i in zip(q1_list, q1_list[1:])])\n",
    "    q2_2gram = set([i for i in zip(q2_list, q2_list[1:])])\n",
    "\n",
    "    shared_2gram = q1_2gram.intersection(q2_2gram)\n",
    "\n",
    "    shared_words = q1words.intersection(q2words)\n",
    "    shared_weights = [weights.get(w, 0) for w in shared_words]\n",
    "    q1_weights = [weights.get(w, 0) for w in q1words]\n",
    "    q2_weights = [weights.get(w, 0) for w in q2words]\n",
    "    total_weights = q1_weights + q1_weights\n",
    "\n",
    "    R1 = np.sum(shared_weights) / np.sum(total_weights) #tfidf share\n",
    "    R2 = len(shared_words) / (len(q1words) + len(q2words) - len(shared_words)) #count share\n",
    "    R31 = len(q1stops) / len(q1words) #stops in q1\n",
    "    R32 = len(q2stops) / len(q2words) #stops in q2\n",
    "    Rcosine_denominator = (np.sqrt(np.dot(q1_weights,q1_weights))*np.sqrt(np.dot(q2_weights,q2_weights)))\n",
    "    Rcosine = np.dot(shared_weights, shared_weights)/Rcosine_denominator\n",
    "    if len(q1_2gram) + len(q2_2gram) == 0:\n",
    "        R2gram = 0\n",
    "    else:\n",
    "        R2gram = len(shared_2gram) / (len(q1_2gram) + len(q2_2gram))\n",
    "    return '{}:{}:{}:{}:{}:{}:{}:{}'.format(R1, R2, len(shared_words), R31, R32, R2gram, Rcosine, words_hamming)\n",
    "x = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'word_match', u'word_match_2root', u'tfidf_word_match',\n",
      "       u'shared_count', u'stops1_ratio', u'stops2_ratio', u'shared_2gram',\n",
      "       u'cosine', u'words_hamming', u'diff_stops_r', u'len_q1', u'len_q2',\n",
      "       u'diff_len', u'caps_count_q1', u'caps_count_q2', u'diff_caps',\n",
      "       u'len_char_q1', u'len_char_q2', u'diff_len_char', u'len_word_q1',\n",
      "       u'len_word_q2', u'diff_len_word', u'avg_world_len1', u'avg_world_len2',\n",
      "       u'diff_avg_word', u'exactly_same', u'duplicated', u'q1_how', u'q2_how',\n",
      "       u'how_both', u'q1_what', u'q2_what', u'what_both', u'q1_which',\n",
      "       u'q2_which', u'which_both', u'q1_who', u'q2_who', u'who_both',\n",
      "       u'q1_where', u'q2_where', u'where_both', u'q1_when', u'q2_when',\n",
      "       u'when_both', u'q1_why', u'q2_why', u'why_both'],\n",
      "      dtype='object')\n",
      "       word_match  word_match_2root  tfidf_word_match  shared_count  \\\n",
      "count         1.0               1.0               1.0           1.0   \n",
      "mean          0.0               0.0               0.0           4.0   \n",
      "std           NaN               NaN               NaN           NaN   \n",
      "min           0.0               0.0               0.0           4.0   \n",
      "25%           0.0               0.0               0.0           4.0   \n",
      "50%           0.0               0.0               0.0           4.0   \n",
      "75%           0.0               0.0               0.0           4.0   \n",
      "max           0.0               0.0               0.0           4.0   \n",
      "\n",
      "       stops1_ratio  stops2_ratio  shared_2gram  cosine  words_hamming  \\\n",
      "count           1.0           1.0           1.0     0.0            1.0   \n",
      "mean            0.0           0.0           0.0     NaN            0.0   \n",
      "std             NaN           NaN           NaN     NaN            NaN   \n",
      "min             0.0           0.0           0.0     NaN            0.0   \n",
      "25%             0.0           0.0           0.0     NaN            0.0   \n",
      "50%             0.0           0.0           0.0     NaN            0.0   \n",
      "75%             0.0           0.0           0.0     NaN            0.0   \n",
      "max             0.0           0.0           0.0     NaN            0.0   \n",
      "\n",
      "       diff_stops_r    ...     who_both  q1_where  q2_where  where_both  \\\n",
      "count           1.0    ...          1.0       1.0       1.0         1.0   \n",
      "mean            0.0    ...          0.0       0.0       0.0         0.0   \n",
      "std             NaN    ...          NaN       NaN       NaN         NaN   \n",
      "min             0.0    ...          0.0       0.0       0.0         0.0   \n",
      "25%             0.0    ...          0.0       0.0       0.0         0.0   \n",
      "50%             0.0    ...          0.0       0.0       0.0         0.0   \n",
      "75%             0.0    ...          0.0       0.0       0.0         0.0   \n",
      "max             0.0    ...          0.0       0.0       0.0         0.0   \n",
      "\n",
      "       q1_when  q2_when  when_both  q1_why  q2_why  why_both  \n",
      "count      1.0      1.0        1.0     1.0     1.0       1.0  \n",
      "mean       0.0      0.0        0.0     0.0     0.0       0.0  \n",
      "std        NaN      NaN        NaN     NaN     NaN       NaN  \n",
      "min        0.0      0.0        0.0     0.0     0.0       0.0  \n",
      "25%        0.0      0.0        0.0     0.0     0.0       0.0  \n",
      "50%        0.0      0.0        0.0     0.0     0.0       0.0  \n",
      "75%        0.0      0.0        0.0     0.0     0.0       0.0  \n",
      "max        0.0      0.0        0.0     0.0     0.0       0.0  \n",
      "\n",
      "[8 rows x 48 columns]\n",
      "Features: ['word_match', 'word_match_2root', 'tfidf_word_match', 'shared_count', 'stops1_ratio', 'stops2_ratio', 'shared_2gram', 'cosine', 'words_hamming', 'diff_stops_r', 'len_q1', 'len_q2', 'diff_len', 'caps_count_q1', 'caps_count_q2', 'diff_caps', 'len_char_q1', 'len_char_q2', 'diff_len_char', 'len_word_q1', 'len_word_q2', 'diff_len_word', 'avg_world_len1', 'avg_world_len2', 'diff_avg_word', 'exactly_same', 'duplicated', 'q1_how', 'q2_how', 'how_both', 'q1_what', 'q2_what', 'what_both', 'q1_which', 'q2_which', 'which_both', 'q1_who', 'q2_who', 'who_both', 'q1_where', 'q2_where', 'where_both', 'q1_when', 'q2_when', 'when_both', 'q1_why', 'q2_why', 'why_both']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/houm/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:85: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "/home/houm/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:90: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.61323014])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'finalized_model.sav'\n",
    "# pickle.dump(clr, open(filename, 'wb'))\n",
    "\n",
    "with open(filename, 'rb') as f:\n",
    "    clr = pickle.load(f)\n",
    "df['word_shares']=word_shares(df)\n",
    "\n",
    "x = pd.DataFrame()\n",
    "\n",
    "x['word_match']       = df['word_shares'].apply(lambda x: float(x.split(':')[0]))\n",
    "x['word_match_2root'] = np.sqrt(x['word_match'])\n",
    "x['tfidf_word_match'] = df['word_shares'].apply(lambda x: float(x.split(':')[1]))\n",
    "x['shared_count']     = df['word_shares'].apply(lambda x: float(x.split(':')[2]))\n",
    "\n",
    "x['stops1_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[3]))\n",
    "x['stops2_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[4]))\n",
    "x['shared_2gram']     = df['word_shares'].apply(lambda x: float(x.split(':')[5]))\n",
    "x['cosine']           = df['word_shares'].apply(lambda x: float(x.split(':')[6]))\n",
    "x['words_hamming']    = df['word_shares'].apply(lambda x: float(x.split(':')[7]))\n",
    "x['diff_stops_r']     = x['stops1_ratio'] - x['stops2_ratio']\n",
    "\n",
    "x['len_q1'] = df['question1'].apply(lambda x: len(str(x)))\n",
    "x['len_q2'] = df['question2'].apply(lambda x: len(str(x)))\n",
    "x['diff_len'] = x['len_q1'] - x['len_q2']\n",
    "\n",
    "x['caps_count_q1'] = df['question1'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "x['caps_count_q2'] = df['question2'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "x['diff_caps'] = x['caps_count_q1'] - x['caps_count_q2']\n",
    "\n",
    "x['len_char_q1'] = df['question1'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "x['len_char_q2'] = df['question2'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "x['diff_len_char'] = x['len_char_q1'] - x['len_char_q2']\n",
    "\n",
    "x['len_word_q1'] = df['question1'].apply(lambda x: len(str(x).split()))\n",
    "x['len_word_q2'] = df['question2'].apply(lambda x: len(str(x).split()))\n",
    "x['diff_len_word'] = x['len_word_q1'] - x['len_word_q2']\n",
    "\n",
    "x['avg_world_len1'] = x['len_char_q1'] / x['len_word_q1']\n",
    "x['avg_world_len2'] = x['len_char_q2'] / x['len_word_q2']\n",
    "x['diff_avg_word'] = x['avg_world_len1'] - x['avg_world_len2']\n",
    "\n",
    "x['exactly_same'] = (df['question1'] == df['question2']).astype(int)\n",
    "x['duplicated'] = df.duplicated(['question1','question2']).astype(int)\n",
    "add_word_count(x, df,'how')\n",
    "add_word_count(x, df,'what')\n",
    "add_word_count(x, df,'which')\n",
    "add_word_count(x, df,'who')\n",
    "add_word_count(x, df,'where')\n",
    "add_word_count(x, df,'when')\n",
    "add_word_count(x, df,'why')\n",
    "\n",
    "print(x.columns)\n",
    "print(x.describe())\n",
    "\n",
    "feature_names = list(x.columns.values)\n",
    "print(\"Features: {}\".format(feature_names))\n",
    "clr.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0',\n",
       " 'bdap?',\n",
       " 'dtype:',\n",
       " 'fees',\n",
       " 'is',\n",
       " 'name:',\n",
       " 'object',\n",
       " 'of',\n",
       " 'question1,',\n",
       " 'what'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(str(df['question1']).lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/houm/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:85: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "/home/houm/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:90: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0:0:4:0:0:0:nan:0'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_shares(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/houm/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:85: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "/home/houm/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:90: RuntimeWarning: invalid value encountered in divide\n"
     ]
    }
   ],
   "source": [
    "df['word_shares']=word_shares(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_match</th>\n",
       "      <th>word_match_2root</th>\n",
       "      <th>tfidf_word_match</th>\n",
       "      <th>shared_count</th>\n",
       "      <th>stops1_ratio</th>\n",
       "      <th>stops2_ratio</th>\n",
       "      <th>shared_2gram</th>\n",
       "      <th>cosine</th>\n",
       "      <th>words_hamming</th>\n",
       "      <th>diff_stops_r</th>\n",
       "      <th>...</th>\n",
       "      <th>who_both</th>\n",
       "      <th>q1_where</th>\n",
       "      <th>q2_where</th>\n",
       "      <th>where_both</th>\n",
       "      <th>q1_when</th>\n",
       "      <th>q2_when</th>\n",
       "      <th>when_both</th>\n",
       "      <th>q1_why</th>\n",
       "      <th>q2_why</th>\n",
       "      <th>why_both</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_match  word_match_2root  tfidf_word_match  shared_count  stops1_ratio  \\\n",
       "0         0.0               0.0               0.0           4.0           0.0   \n",
       "\n",
       "   stops2_ratio  shared_2gram  cosine  words_hamming  diff_stops_r    ...     \\\n",
       "0           0.0           0.0     NaN            0.0           0.0    ...      \n",
       "\n",
       "   who_both  q1_where  q2_where  where_both  q1_when  q2_when  when_both  \\\n",
       "0         0         0         0           0        0        0          0   \n",
       "\n",
       "   q1_why  q2_why  why_both  \n",
       "0       0       0         0  \n",
       "\n",
       "[1 rows x 48 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61323014])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clr.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
